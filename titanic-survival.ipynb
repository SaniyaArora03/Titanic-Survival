{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Load the Titanic train and test datasets\n",
    "train = pd.read_csv(\"/kaggle/input/titanic-dataset/Titanic-Dataset.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/titanic-dataset/Titanic-Dataset.csv\")\n",
    "\n",
    "#.shape shows number of rows and colums in dataset\n",
    "print(\"train shape:\", train.shape) \n",
    "print(\"test shape:\", test.shape)\n",
    "\n",
    "#shows column names, non-null values,data types\n",
    "train.info()\n",
    " \n",
    "print(\"\\nMissing values (train):\\n\", train.isnull().sum())    #counts sum of null values\n",
    "print(\"\\nSurvival value counts:\\n\", train['Survived'].value_counts(normalize=True))  \n",
    "#train['Survived'] → Selects the target column (0 = died, 1 = survived).\n",
    "#.value_counts(normalize=True) → Shows percentage distribution instead of raw counts.\n",
    "#Helps detect class imbalance.\n",
    "\n",
    "#VISUALISE SURVIVAL COUNTS\n",
    "sns.countplot(x='Survived', data=train)\n",
    "plt.title('Survived counts')\n",
    "plt.show()\n",
    "#sns.countplot → Makes a bar chart showing count of passengers who survived vs. didn’t survive.\n",
    "#plt.title() → Adds a title.\n",
    "#.show() → Displays the plot.\n",
    "\n",
    "#VISUALISE SURVIVAL RATE BY GENDER\n",
    "sns.countplot(x='Sex',hue='Survived',data=train)\n",
    "\n",
    "#AGE DISTRIBUTION BY PASSENGER CLASS\n",
    "sns,boxplot(x='Pclass',y='Age',data=train)\n",
    "plt.title('Age by Pclass')\n",
    "plt.show()\n",
    "\n",
    "#DATA PREPROCESSING - FEATURE ENGINEERING\n",
    "# Step 3 - simple cleaning\n",
    "train2 = train.copy()\n",
    "test2 = test.copy()\n",
    "\n",
    "# Save PassengerId for final submission\n",
    "test_passenger_ids = test2['PassengerId']\n",
    "\n",
    "# Fill common missing values (simple baseline)\n",
    "train2['Age'].fillna(train2['Age'].median(), inplace=True)\n",
    "test2['Age'].fillna(test2['Age'].median(), inplace=True)\n",
    "train2['Embarked'].fillna(train2['Embarked'].mode()[0], inplace=True)\n",
    "test2['Fare'].fillna(test2['Fare'].median(), inplace=True)\n",
    "\n",
    "# Drop Cabin (lots missing) and Name/Ticket/PassengerId for model input\n",
    "train2.drop(['Cabin'], axis=1, inplace=True)\n",
    "test2.drop(['Cabin'], axis=1, inplace=True)\n",
    "\n",
    "full = pd.concat([train2.assign(dataset='train'), test2.assign(dataset='test')], sort=False)\n",
    "#concats both datasets and assigns which is trained and which is test, sort=false keeps it in same order without sorting\n",
    "\n",
    "full['Title'] = full['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
    "#Goal: Captures social title, which is correlated with age, gender, and survival odds.\n",
    "\n",
    "# Map rare titles to common groups\n",
    "#Prevents very small categories that could confuse the model.\n",
    "title_map = {\n",
    "    'Mlle':'Miss', 'Ms':'Miss', 'Mme':'Mrs',\n",
    "    'Lady':'Rare','Countess':'Rare','Capt':'Rare','Col':'Rare','Don':'Rare','Dr':'Rare','Major':'Rare',\n",
    "    'Rev':'Rare','Sir':'Rare','Jonkheer':'Rare','Dona':'Rare'\n",
    "}\n",
    "full['Title'] = full['Title'].replace(title_map)\n",
    "\n",
    "#\n",
    "full['FamilySize'] = full['SibSp'] + full['Parch'] + 1  \n",
    "full['IsAlone'] = (full['FamilySize']==1).astype(int)\n",
    "#SibSp = siblings/spouses aboard, Parch = parents/children aboard.\n",
    "#+1 includes the passenger themselves.\n",
    "#IsAlone = binary flag → 1 if no family aboard, else 0.\n",
    "\n",
    "full['Age'] = full.groupby('Title')['Age'].transform(lambda x: x.fillna(x.median()))\n",
    "#Groups passengers by Title (e.g., Mr, Miss, Rare).\n",
    "#Fills missing ages with the median age for that title.\n",
    "\n",
    "full['Fare'] = full.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "#Turns each category into binary columns (0/1).\n",
    "#drop_first=True → Avoids multicollinearity by dropping one category from each set.\n",
    "#Example: Sex → only Sex_male (female implied when 0).\n",
    "full = pd.get_dummies(full, columns=['Sex','Embarked','Title'], drop_first=True)\n",
    "\n",
    "#drop unused non numeric colums\n",
    "full.drop(['Name','Ticket','PassengerId','dataset'],axis=1,inplace=True)\n",
    "\n",
    "train_fe=full.loc[full.loc[full['Survived'].notnull()].copy()]\n",
    "test_fe = full.loc[full['Survived'].isnull()].drop('Survived', axis=1).copy()\n",
    "#train_fe: Rows with Survived value (original training set).\n",
    "#test_fe: Rows without Survived (original test set).\n",
    "#Drop Survived from test since we’ll predict it.\n",
    "\n",
    "X = train_fe.drop('Survived', axis=1)\n",
    "y = train_fe['Survived'].astype(int)\n",
    "#X: All columns used to predict survival and drop the actual survival column ie features\n",
    "#y:the actual survival column \n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"test_fe shape:\", test_fe.shape)\n",
    "\n",
    "#ACTUAL MODEL TRAINING\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import  accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "#splitting data into training and validation\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.2,random_state=42, stratify=y)\n",
    "\n",
    "#LOGISTIC REGRESSION\n",
    "lr=LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train,y_train)\n",
    "print(\"LR val acc:\", accuracy_score(y_val, lr.predict(X_val)))\n",
    "\n",
    "#RANDOM FOREST\n",
    "rf=RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "yhat = rf.predict(X_val)\n",
    "\n",
    "#accuracy score\n",
    "print(\"RF val acc:\", accuracy_score(y_val, yhat)) \n",
    "print(classification_report(y_val,yhat))\n",
    "\n",
    "#CROSS VALIDATION\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#purpose:to check how well random forest model works to detect overfitting/underfitting\n",
    "print(\"RF CV mean:\", cross_val_score(rf, X, y, cv=5).mean())\n",
    "cross_val_score → Performs cross-validation.\n",
    "\n",
    "#rf → This is your Random Forest model (already trained or ready to train).\n",
    "#X → Features (input data without the target).\n",
    "#y → Target variable (Survived in Titanic).\n",
    "#cv=5 → 5-fold cross-validation:\n",
    "   #The dataset is split into 5 equal parts.\n",
    "   #The model trains on 4 parts and tests on the remaining 1 part.\n",
    "   #This repeats 5 times so every part gets tested once.\n",
    "   #.mean() → Takes the average of the 5 test scores to give a more reliable accuracy measure than a single train-test split.\n",
    "\n",
    "importances=pd.Series(rf.feature_importances_,index=X.columns).sort_values(ascending=False))\n",
    "#rf.feature_importances_ → After training, Random Forest can tell you how important each feature was for making predictions.\n",
    "#pd.Series(..., index=X.columns) → Makes a Pandas Series where:\n",
    "   #Values = importance scores from the model\n",
    "   #Index = feature names\n",
    "   #.sort_values(ascending=False) → Sorts so the most important feature is at the top.\n",
    "\n",
    "test_fe_aligned = test_fe.reindex(columns=X.columns, fill_value=0)\n",
    "#The test DataFrame has exactly the same columns as X (training features).\n",
    "#If a column is missing, it fills it with 0 (meaning that category did not occur for that row).\n",
    "\n",
    "#training final model\n",
    "final_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "final_model.fit(X, y)\n",
    "\n",
    "pred_test = final_model.predict(test_fe_aligned)\n",
    "\n",
    "\n",
    "#final result whether passenger survived or not\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_passenger_ids,\n",
    "    'Survived': pred_test.astype(int)\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Saved submission.csv — upload or submit from Kaggle notebook UI\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1818188,
     "sourceId": 2965537,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
